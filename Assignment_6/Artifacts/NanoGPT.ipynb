{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NanoGPT\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "szsoZo0zQVlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the nanoGPT repository\n",
        "!git clone https://github.com/karpathy/nanoGPT.git\n",
        "%cd nanoGPT\n",
        "\n",
        "# Install required packages\n",
        "!pip install numpy transformers datasets tiktoken wandb tqdm\n",
        "\n",
        "# Upgrade PyTorch to version 2.0.1 with CUDA 11.8 support\n",
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n"
      ],
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Shakespeare's Works and Create Data Directory"
      ],
      "metadata": {
        "id": "RcxxLWAkrP7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Shakespeare's works\n",
        "!wget https://www.gutenberg.org/files/100/100-0.txt -O shakespeare.txt\n",
        "\n",
        "# Create a directory for the data\n",
        "!mkdir data/shakespeare\n",
        "\n",
        "# Move the text file to the data directory\n",
        "!mv shakespeare.txt data/shakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNPdsT4OSAVx",
        "outputId": "58dbc66d-960c-4353-990b-302b1cd0cfc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-18 22:29:16--  https://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5618733 (5.4M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "shakespeare.txt     100%[===================>]   5.36M  2.66MB/s    in 2.0s    \n",
            "\n",
            "2024-11-18 22:29:19 (2.66 MB/s) - ‘shakespeare.txt’ saved [5618733/5618733]\n",
            "\n",
            "mkdir: cannot create directory ‘data/shakespeare’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preperation"
      ],
      "metadata": {
        "id": "r5PSEyuorTa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the prepare.py script\n",
        "%%writefile data/shakespeare/prepare.py\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Define input and output paths\n",
        "input_file_path = 'data/shakespeare/input.txt'\n",
        "train_output_file_path = 'data/shakespeare/train.bin'\n",
        "val_output_file_path = 'data/shakespeare/val.bin'\n",
        "\n",
        "# Read the input text\n",
        "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "    data = f.read()\n",
        "\n",
        "# Get all unique characters\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Unique characters: {vocab_size}\")\n",
        "\n",
        "# Create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# Encode the entire text data\n",
        "data_size = len(data)\n",
        "print(f\"Data has {data_size} characters.\")\n",
        "\n",
        "# Convert data to integers\n",
        "encoded_data = np.array([stoi[c] for c in data], dtype=np.uint16)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "n = int(0.9 * len(encoded_data))\n",
        "train_data = encoded_data[:n]\n",
        "val_data = encoded_data[n:]\n",
        "\n",
        "# Save the data to .bin files\n",
        "train_data.tofile(train_output_file_path)\n",
        "val_data.tofile(val_output_file_path)\n",
        "\n",
        "# Save the mapping for decoding\n",
        "meta = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'itos': itos,\n",
        "    'stoi': stoi,\n",
        "}\n",
        "with open('data/shakespeare/meta.pkl', 'wb') as f:\n",
        "    pickle.dump(meta, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ijcyeLgSATd",
        "outputId": "b4aadd7d-1ce1-406f-fbc6-45cccb8da2f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data/shakespeare/prepare.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the data preparation script\n",
        "!python data/shakespeare/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAB-fsglSARL",
        "outputId": "3f5714fa-09af-4e8b-fbce-2fee7f17b5af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique characters: 100\n",
            "Data has 5359439 characters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the shakespeare.py config file\n",
        "%%writefile config/shakespeare.py\n",
        "\n",
        "# Configuration for training nanoGPT on Shakespeare's works\n",
        "\n",
        "out_dir = 'out-shakespeare'\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "log_interval = 100\n",
        "\n",
        "always_save_checkpoint = False\n",
        "\n",
        "wandb_log = False  # Set to True if using Weights & Biases\n",
        "wandb_project = 'shakespeare'\n",
        "wandb_run_name = 'mini-gpt'\n",
        "\n",
        "dataset = 'shakespeare'\n",
        "batch_size = 64\n",
        "block_size = 256  # Context length\n",
        "\n",
        "# Model configuration\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 384\n",
        "dropout = 0.2\n",
        "\n",
        "learning_rate = 1e-3\n",
        "max_iters = 5000\n",
        "lr_decay_iters = 5000  # Make equal to max_iters\n",
        "min_lr = 1e-4\n",
        "beta2 = 0.99\n",
        "\n",
        "warmup_iters = 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkWwKY5jSAOz",
        "outputId": "372c1e73-b99f-4a68-cca6-9eb17c5dec56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config/shakespeare.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if CUDA is Available"
      ],
      "metadata": {
        "id": "u0_clRpmrdWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"CUDA is available. GPU device name: {gpu_name}\")\n",
        "else:\n",
        "    print(\"CUDA is not available. No GPU detected.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCmNoj1JZyUQ",
        "outputId": "b91871bd-327e-443f-ed08-e14e3d557c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. GPU device name: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Training"
      ],
      "metadata": {
        "id": "1MzSkvLUrfoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "!python train.py config/shakespeare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3tl-VWGSAMn",
        "outputId": "e039f60a-ecb4-48d6-d7cc-28f616280464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/shakespeare.py:\n",
            "\n",
            "# Configuration for training nanoGPT on Shakespeare's works\n",
            "\n",
            "out_dir = 'out-shakespeare'\n",
            "eval_interval = 500\n",
            "eval_iters = 200\n",
            "log_interval = 100\n",
            "\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False  # Set to True if using Weights & Biases\n",
            "wandb_project = 'shakespeare'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare'\n",
            "batch_size = 64\n",
            "block_size = 256  # Context length\n",
            "\n",
            "# Model configuration\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000  # Make equal to max_iters\n",
            "min_lr = 1e-4\n",
            "beta2 = 0.99\n",
            "\n",
            "warmup_iters = 100\n",
            "\n",
            "tokens per iteration will be: 655,360\n",
            "found vocab_size = 100 (inside data/shakespeare/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.66M\n",
            "num decayed parameter tensors: 26, with 10,753,536 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.6115, val loss 4.6009\n",
            "[2024-11-18 22:29:38,800] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:39,307] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:40,189] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:40,493] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:40,949] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:41,250] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:41,707] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:42,007] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:42,453] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:42,907] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:43,362] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:43,663] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 0: loss 4.6308, time 21692.12ms, mfu -100.00%\n",
            "iter 100: loss 2.4997, time 893.04ms, mfu 16.71%\n",
            "iter 200: loss 2.0246, time 900.96ms, mfu 16.69%\n",
            "iter 300: loss 1.6570, time 899.31ms, mfu 16.68%\n",
            "iter 400: loss 1.4684, time 895.68ms, mfu 16.68%\n",
            "step 500: train loss 1.2701, val loss 1.4630\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 500: loss 1.3204, time 7198.56ms, mfu 15.22%\n",
            "iter 600: loss 1.3048, time 929.46ms, mfu 15.30%\n",
            "iter 700: loss 1.2122, time 917.05ms, mfu 15.40%\n",
            "iter 800: loss 1.1784, time 903.99ms, mfu 15.51%\n",
            "iter 900: loss 1.1693, time 902.28ms, mfu 15.61%\n",
            "step 1000: train loss 1.0517, val loss 1.3474\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 1000: loss 1.1502, time 4313.46ms, mfu 14.40%\n",
            "iter 1100: loss 1.1157, time 908.82ms, mfu 14.60%\n",
            "iter 1200: loss 1.1073, time 907.76ms, mfu 14.78%\n",
            "iter 1300: loss 1.0460, time 897.15ms, mfu 14.97%\n",
            "iter 1400: loss 1.0436, time 911.81ms, mfu 15.11%\n",
            "step 1500: train loss 0.9231, val loss 1.3518\n",
            "iter 1500: loss 1.0272, time 4013.84ms, mfu 13.97%\n",
            "iter 1600: loss 1.0094, time 903.10ms, mfu 14.23%\n",
            "iter 1700: loss 0.9933, time 919.60ms, mfu 14.43%\n",
            "iter 1800: loss 0.9963, time 908.53ms, mfu 14.63%\n",
            "iter 1900: loss 0.9814, time 893.54ms, mfu 14.83%\n",
            "step 2000: train loss 0.8307, val loss 1.3742\n",
            "iter 2000: loss 1.0020, time 4017.72ms, mfu 13.72%\n",
            "iter 2100: loss 0.9584, time 905.38ms, mfu 14.00%\n",
            "iter 2200: loss 0.9765, time 893.55ms, mfu 14.27%\n",
            "iter 2300: loss 0.9403, time 906.65ms, mfu 14.49%\n",
            "iter 2400: loss 0.9351, time 903.65ms, mfu 14.69%\n",
            "step 2500: train loss 0.7598, val loss 1.4128\n",
            "iter 2500: loss 0.9479, time 3994.17ms, mfu 13.59%\n",
            "iter 2600: loss 0.9013, time 900.86ms, mfu 13.89%\n",
            "iter 2700: loss 0.8994, time 894.77ms, mfu 14.17%\n",
            "iter 2800: loss 0.9085, time 905.41ms, mfu 14.40%\n",
            "iter 2900: loss 0.9001, time 893.79ms, mfu 14.63%\n",
            "step 3000: train loss 0.7084, val loss 1.4434\n",
            "iter 3000: loss 0.8846, time 4014.85ms, mfu 13.54%\n",
            "iter 3100: loss 0.8831, time 892.87ms, mfu 13.86%\n",
            "iter 3200: loss 0.8769, time 919.02ms, mfu 14.09%\n",
            "iter 3300: loss 0.8870, time 917.92ms, mfu 14.31%\n",
            "iter 3400: loss 0.8635, time 889.35ms, mfu 14.56%\n",
            "step 3500: train loss 0.6705, val loss 1.4706\n",
            "iter 3500: loss 0.8817, time 3981.11ms, mfu 13.48%\n",
            "iter 3600: loss 0.8882, time 906.61ms, mfu 13.77%\n",
            "iter 3700: loss 0.8470, time 894.03ms, mfu 14.07%\n",
            "iter 3800: loss 0.8534, time 929.74ms, mfu 14.26%\n",
            "iter 3900: loss 0.8459, time 909.17ms, mfu 14.48%\n",
            "step 4000: train loss 0.6387, val loss 1.4888\n",
            "iter 4000: loss 0.8696, time 4020.63ms, mfu 13.40%\n",
            "iter 4100: loss 0.8286, time 899.53ms, mfu 13.72%\n",
            "iter 4200: loss 0.8417, time 908.88ms, mfu 13.99%\n",
            "iter 4300: loss 0.8747, time 908.77ms, mfu 14.23%\n",
            "iter 4400: loss 0.8452, time 898.97ms, mfu 14.47%\n",
            "step 4500: train loss 0.6222, val loss 1.5029\n",
            "iter 4500: loss 0.8300, time 4019.63ms, mfu 13.39%\n",
            "iter 4600: loss 0.8115, time 903.15ms, mfu 13.71%\n",
            "iter 4700: loss 0.8032, time 943.08ms, mfu 13.92%\n",
            "iter 4800: loss 0.8186, time 908.81ms, mfu 14.17%\n",
            "iter 4900: loss 0.8233, time 905.47ms, mfu 14.40%\n",
            "step 5000: train loss 0.6085, val loss 1.5135\n",
            "iter 5000: loss 0.8263, time 4025.69ms, mfu 13.33%\n"
          ]
        }
      ]
    }
  ]
}