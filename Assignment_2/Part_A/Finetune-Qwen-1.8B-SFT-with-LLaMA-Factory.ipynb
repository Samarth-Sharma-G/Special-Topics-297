{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9","timestamp":1726384183354}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Install Dependencies"],"metadata":{"id":"lr7rB3szzhtx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"giM74oK1rRIH","executionInfo":{"status":"ok","timestamp":1726376271043,"user_tz":420,"elapsed":218763,"user":{"displayName":"Samarth Sharma","userId":"03778731126873596478"}},"outputId":"f3c9bad2-58e2-4456-b69a-a64e6d05bce7","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into 'LLaMA-Factory'...\n","remote: Enumerating objects: 315, done.\u001b[K\n","remote: Counting objects: 100% (315/315), done.\u001b[K\n","remote: Compressing objects: 100% (245/245), done.\u001b[K\n","remote: Total 315 (delta 80), reused 184 (delta 57), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (315/315), 8.94 MiB | 10.04 MiB/s, done.\n","Resolving deltas: 100% (80/80), done.\n","/content/LLaMA-Factory\n","\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n","CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n","\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n","Collecting torch==2.3.1\n","  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","Collecting torchvision==0.18.1\n","  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Collecting torchaudio==2.3.1\n","  Downloading torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.16.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1)\n","  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.3.1 (from torch==2.3.1)\n","  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (9.4.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1)\n","  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n","Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.22.3\n","    Uninstalling nvidia-nccl-cu12-2.22.3:\n","      Successfully uninstalled nvidia-nccl-cu12-2.22.3\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.4.0+cu121\n","    Uninstalling torch-2.4.0+cu121:\n","      Successfully uninstalled torch-2.4.0+cu121\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.19.0+cu121\n","    Uninstalling torchvision-0.19.0+cu121:\n","      Successfully uninstalled torchvision-0.19.0+cu121\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 2.4.0+cu121\n","    Uninstalling torchaudio-2.4.0+cu121:\n","      Successfully uninstalled torchaudio-2.4.0+cu121\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 torch-2.3.1 torchaudio-2.3.1 torchvision-0.18.1 triton-2.3.1\n","Found existing installation: jax 0.4.26\n","Uninstalling jax-0.4.26:\n","  Successfully uninstalled jax-0.4.26\n","Obtaining file:///content/LLaMA-Factory\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers<=4.45.0,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (4.44.2)\n","Collecting datasets<=2.21.0,>=2.16.0 (from llamafactory==0.9.1.dev0)\n","  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: accelerate<=0.34.2,>=0.30.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.34.2)\n","Collecting peft<=0.12.0,>=0.11.1 (from llamafactory==0.9.1.dev0)\n","  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n","Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.1.dev0)\n","  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n","Collecting gradio>=4.0.0 (from llamafactory==0.9.1.dev0)\n","  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.1.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (1.13.1)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.8.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.1.99)\n","Collecting tiktoken (from llamafactory==0.9.1.dev0)\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (3.20.3)\n","Collecting uvicorn (from llamafactory==0.9.1.dev0)\n","  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.9.1)\n","Collecting fastapi (from llamafactory==0.9.1.dev0)\n","  Downloading fastapi-0.114.2-py3-none-any.whl.metadata (27 kB)\n","Collecting sse-starlette (from llamafactory==0.9.1.dev0)\n","  Downloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (3.7.1)\n","Collecting fire (from llamafactory==0.9.1.dev0)\n","  Downloading fire-0.6.0.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (24.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (6.0.2)\n","Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (1.26.4)\n","Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.3.1)\n","Collecting liger-kernel (from llamafactory==0.9.1.dev0)\n","  Downloading liger_kernel-0.3.0-py3-none-any.whl.metadata (25 kB)\n","Collecting bitsandbytes>=0.39.0 (from llamafactory==0.9.1.dev0)\n","  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (5.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (0.24.6)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (0.4.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.16.0)\n","Collecting pyarrow>=15.0.0 (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0)\n","  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.66.5)\n","Collecting xxhash (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.10.5)\n","Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.7.1)\n","Collecting ffmpy (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n","  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting gradio-client==1.3.0 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n","  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (6.4.5)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.1.5)\n","Collecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n","  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (9.4.0)\n","Collecting pydub (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n","  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n","Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n","  Downloading ruff-0.6.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.12.5)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (4.12.2)\n","Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.0.7)\n","Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Collecting starlette<0.39.0,>=0.37.2 (from fastapi->llamafactory==0.9.1.dev0)\n","  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.1.dev0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.1.dev0) (2.23.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (3.3)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (2.3.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.9.1.dev0) (12.6.68)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.45.0,>=4.41.2->llamafactory==0.9.1.dev0) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.45.0,>=4.41.2->llamafactory==0.9.1.dev0) (0.19.1)\n","Collecting tyro>=0.5.11 (from trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0)\n","  Downloading tyro-0.8.10-py3-none-any.whl.metadata (8.4 kB)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.1.dev0) (8.1.7)\n","Collecting h11>=0.8 (from uvicorn->llamafactory==0.9.1.dev0)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.1.dev0) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.1.dev0) (2.4.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.8)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.2.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.0.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.9.1.dev0) (2024.8.30)\n","Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.9.1.dev0)\n","  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.3.2)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (13.8.1)\n","Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (0.16)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0)\n","  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.9.1.dev0) (1.3.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.1.2)\n","Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio-4.44.0-py3-none-any.whl (18.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Downloading fastapi-0.114.2-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading liger_kernel-0.3.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\n","Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n","Downloading ruff-0.6.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m124.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading starlette-0.38.5-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tyro-0.8.10-py3-none-any.whl (105 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n","Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llamafactory, fire\n","  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llamafactory: filename=llamafactory-0.9.1.dev0-0.editable-py3-none-any.whl size=22354 sha256=02076114c8ab2b33e308b0c1e01308bceab96510538044eb1709238497b4781c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-e4s7ho0y/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117030 sha256=bb103ac06f18d7d6fc8977535e589c83bdd2fad9170f48033f41aeea999f6f3d\n","  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n","Successfully built llamafactory fire\n","Installing collected packages: pydub, xxhash, websockets, tomlkit, shtab, semantic-version, ruff, python-multipart, pyarrow, orjson, h11, fire, ffmpy, dill, aiofiles, uvicorn, tiktoken, starlette, multiprocess, httpcore, tyro, sse-starlette, httpx, fastapi, gradio-client, bitsandbytes, peft, liger-kernel, gradio, datasets, trl, llamafactory\n","  Attempting uninstall: tomlkit\n","    Found existing installation: tomlkit 0.13.2\n","    Uninstalling tomlkit-0.13.2:\n","      Successfully uninstalled tomlkit-0.13.2\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aiofiles-23.2.1 bitsandbytes-0.43.3 datasets-2.21.0 dill-0.3.8 fastapi-0.114.2 ffmpy-0.4.0 fire-0.6.0 gradio-4.44.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 liger-kernel-0.3.0 llamafactory-0.9.1.dev0 multiprocess-0.70.16 orjson-3.10.7 peft-0.12.0 pyarrow-17.0.0 pydub-0.25.1 python-multipart-0.0.9 ruff-0.6.5 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.1.3 starlette-0.38.5 tiktoken-0.7.0 tomlkit-0.12.0 trl-0.9.6 tyro-0.8.10 uvicorn-0.30.6 websockets-12.0 xxhash-3.5.0\n"]}],"source":["%cd /content/\n","%rm -rf LLaMA-Factory\n","!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n","%cd LLaMA-Factory\n","%ls\n","!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n","!pip uninstall -y jax\n","!pip install -e .[torch,bitsandbytes,liger-kernel]"]},{"cell_type":"markdown","source":["### Check GPU environment"],"metadata":{"id":"H9RXn_YQnn9f"}},{"cell_type":"code","source":["import torch\n","try:\n","  assert torch.cuda.is_available() is True\n","except AssertionError:\n","  print(\"Please set up a GPU before using LLaMA Factory\")"],"metadata":{"id":"ZkN-ktlsnrdU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"id":"mcNcHcA4bf4Z","executionInfo":{"status":"ok","timestamp":1726376761075,"user_tz":420,"elapsed":8054,"user":{"displayName":"Samarth Sharma","userId":"03778731126873596478"}},"outputId":"8a7f9c81-e85b-47bf-cefa-30ca8341cbbe","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n","    Setting a new token will erase the existing one.\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"markdown","source":["## Fine-tune model via LLaMA Board"],"metadata":{"id":"2QiXcvdzzW3Y"}},{"cell_type":"code","source":["%cd /content/LLaMA-Factory/\n","!GRADIO_SHARE=1 llamafactory-cli webui"],"metadata":{"id":"YLsdS6V5yUMy","outputId":"3b326de0-273d-4309-e1b4-176478a1ad48","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/LLaMA-Factory\n","2024-09-15 07:05:46.213785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-09-15 07:05:46.234873: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-09-15 07:05:46.241377: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-09-15 07:05:46.257873: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-09-15 07:05:47.477618: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Running on local URL:  http://0.0.0.0:7860\n","Running on public URL: https://4c9936d67f0d482c39.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","config.json: 100% 910/910 [00:00<00:00, 5.56MB/s]\n","[INFO|configuration_utils.py:733] 2024-09-15 07:06:31,475 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n","configuration_qwen.py: 100% 2.35k/2.35k [00:00<00:00, 14.1MB/s]\n","[INFO|configuration_utils.py:733] 2024-09-15 07:06:31,766 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n","[INFO|configuration_utils.py:800] 2024-09-15 07:06:31,767 >> Model config QWenConfig {\n","  \"_name_or_path\": \"Qwen/Qwen-1_8B-Chat\",\n","  \"architectures\": [\n","    \"QWenLMHeadModel\"\n","  ],\n","  \"attn_dropout_prob\": 0.0,\n","  \"auto_map\": {\n","    \"AutoConfig\": \"Qwen/Qwen-1_8B-Chat--configuration_qwen.QWenConfig\",\n","    \"AutoModelForCausalLM\": \"Qwen/Qwen-1_8B-Chat--modeling_qwen.QWenLMHeadModel\"\n","  },\n","  \"bf16\": false,\n","  \"emb_dropout_prob\": 0.0,\n","  \"fp16\": false,\n","  \"fp32\": false,\n","  \"hidden_size\": 2048,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"kv_channels\": 128,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"max_position_embeddings\": 8192,\n","  \"model_type\": \"qwen\",\n","  \"no_bias\": true,\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"onnx_safe\": null,\n","  \"rotary_emb_base\": 10000,\n","  \"rotary_pct\": 1.0,\n","  \"scale_attn_weights\": true,\n","  \"seq_length\": 8192,\n","  \"softmax_in_fp32\": false,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"QWenTokenizer\",\n","  \"transformers_version\": \"4.44.2\",\n","  \"use_cache\": true,\n","  \"use_cache_kernel\": false,\n","  \"use_cache_quantization\": false,\n","  \"use_dynamic_ntk\": true,\n","  \"use_flash_attn\": \"auto\",\n","  \"use_logn_attn\": true,\n","  \"vocab_size\": 151936\n","}\n","\n","tokenizer_config.json: 100% 173/173 [00:00<00:00, 904kB/s]\n","tokenization_qwen.py: 100% 9.62k/9.62k [00:00<00:00, 25.2MB/s]\n","qwen.tiktoken: 100% 2.56M/2.56M [00:00<00:00, 7.67MB/s]\n","[INFO|tokenization_utils_base.py:2269] 2024-09-15 07:06:33,165 >> loading file qwen.tiktoken from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/qwen.tiktoken\n","[INFO|tokenization_utils_base.py:2269] 2024-09-15 07:06:33,165 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2269] 2024-09-15 07:06:33,165 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2269] 2024-09-15 07:06:33,165 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2269] 2024-09-15 07:06:33,165 >> loading file tokenizer.json from cache at None\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","[INFO|configuration_utils.py:733] 2024-09-15 07:06:33,968 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n","[INFO|configuration_utils.py:733] 2024-09-15 07:06:34,153 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n","[INFO|configuration_utils.py:800] 2024-09-15 07:06:34,154 >> Model config QWenConfig {\n","  \"_name_or_path\": \"Qwen/Qwen-1_8B-Chat\",\n","  \"architectures\": [\n","    \"QWenLMHeadModel\"\n","  ],\n","  \"attn_dropout_prob\": 0.0,\n","  \"auto_map\": {\n","    \"AutoConfig\": \"Qwen/Qwen-1_8B-Chat--configuration_qwen.QWenConfig\",\n","    \"AutoModelForCausalLM\": \"Qwen/Qwen-1_8B-Chat--modeling_qwen.QWenLMHeadModel\"\n","  },\n","  \"bf16\": false,\n","  \"emb_dropout_prob\": 0.0,\n","  \"fp16\": false,\n","  \"fp32\": false,\n","  \"hidden_size\": 2048,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"kv_channels\": 128,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"max_position_embeddings\": 8192,\n","  \"model_type\": \"qwen\",\n","  \"no_bias\": true,\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"onnx_safe\": null,\n","  \"rotary_emb_base\": 10000,\n","  \"rotary_pct\": 1.0,\n","  \"scale_attn_weights\": true,\n","  \"seq_length\": 8192,\n","  \"softmax_in_fp32\": false,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"QWenTokenizer\",\n","  \"transformers_version\": \"4.44.2\",\n","  \"use_cache\": true,\n","  \"use_cache_kernel\": false,\n","  \"use_cache_quantization\": false,\n","  \"use_dynamic_ntk\": true,\n","  \"use_flash_attn\": \"auto\",\n","  \"use_logn_attn\": true,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|tokenization_utils_base.py:2269] 2024-09-15 07:06:34,340 >> loading file qwen.tiktoken from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/qwen.tiktoken\n","[INFO|tokenization_utils_base.py:2269] 2024-09-15 07:06:34,340 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2269] 2024-09-15 07:06:34,340 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2269] 2024-09-15 07:06:34,340 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2269] 2024-09-15 07:06:34,340 >> loading file tokenizer.json from cache at None\n","09/15/2024 07:06:35 - INFO - llamafactory.data.template - Add eos token: <|im_end|>\n","09/15/2024 07:06:35 - INFO - llamafactory.data.template - Add pad token: <|im_end|>\n","[INFO|configuration_utils.py:733] 2024-09-15 07:06:35,216 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n","[INFO|configuration_utils.py:733] 2024-09-15 07:06:35,403 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n","[INFO|configuration_utils.py:800] 2024-09-15 07:06:35,404 >> Model config QWenConfig {\n","  \"_name_or_path\": \"Qwen/Qwen-1_8B-Chat\",\n","  \"architectures\": [\n","    \"QWenLMHeadModel\"\n","  ],\n","  \"attn_dropout_prob\": 0.0,\n","  \"auto_map\": {\n","    \"AutoConfig\": \"Qwen/Qwen-1_8B-Chat--configuration_qwen.QWenConfig\",\n","    \"AutoModelForCausalLM\": \"Qwen/Qwen-1_8B-Chat--modeling_qwen.QWenLMHeadModel\"\n","  },\n","  \"bf16\": false,\n","  \"emb_dropout_prob\": 0.0,\n","  \"fp16\": false,\n","  \"fp32\": false,\n","  \"hidden_size\": 2048,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"kv_channels\": 128,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"max_position_embeddings\": 8192,\n","  \"model_type\": \"qwen\",\n","  \"no_bias\": true,\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"onnx_safe\": null,\n","  \"rotary_emb_base\": 10000,\n","  \"rotary_pct\": 1.0,\n","  \"scale_attn_weights\": true,\n","  \"seq_length\": 8192,\n","  \"softmax_in_fp32\": false,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"QWenTokenizer\",\n","  \"transformers_version\": \"4.44.2\",\n","  \"use_cache\": true,\n","  \"use_cache_kernel\": false,\n","  \"use_cache_quantization\": false,\n","  \"use_dynamic_ntk\": true,\n","  \"use_flash_attn\": \"auto\",\n","  \"use_logn_attn\": true,\n","  \"vocab_size\": 151936\n","}\n","\n","09/15/2024 07:06:35 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n","modeling_qwen.py: 100% 55.6k/55.6k [00:00<00:00, 144MB/s]\n","cpp_kernels.py: 100% 1.92k/1.92k [00:00<00:00, 10.9MB/s]\n","qwen_generation_utils.py: 100% 14.6k/14.6k [00:00<00:00, 57.7MB/s]\n","model.safetensors.index.json: 100% 14.7k/14.7k [00:00<00:00, 55.8MB/s]\n","[INFO|modeling_utils.py:3678] 2024-09-15 07:06:36,608 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/model.safetensors.index.json\n","Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n","model-00001-of-00002.safetensors:   0% 0.00/2.04G [00:00<?, ?B/s]\u001b[A\n","model-00001-of-00002.safetensors:   1% 10.5M/2.04G [00:00<01:29, 22.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   1% 21.0M/2.04G [00:00<01:00, 33.4MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   2% 31.5M/2.04G [00:00<00:47, 42.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   2% 41.9M/2.04G [00:00<00:37, 53.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   3% 62.9M/2.04G [00:01<00:25, 76.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   4% 83.9M/2.04G [00:01<00:20, 96.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   5% 105M/2.04G [00:01<00:17, 112MB/s]  \u001b[A\n","model-00001-of-00002.safetensors:   6% 126M/2.04G [00:01<00:14, 133MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   7% 147M/2.04G [00:01<00:12, 146MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   8% 168M/2.04G [00:01<00:11, 156MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   9% 189M/2.04G [00:01<00:11, 167MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  10% 210M/2.04G [00:01<00:10, 177MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  12% 241M/2.04G [00:02<00:09, 186MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  13% 273M/2.04G [00:02<00:09, 195MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  14% 294M/2.04G [00:02<00:09, 188MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  15% 315M/2.04G [00:02<00:08, 193MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  16% 336M/2.04G [00:02<00:08, 195MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  17% 357M/2.04G [00:02<00:08, 190MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  19% 377M/2.04G [00:02<00:08, 195MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  20% 398M/2.04G [00:08<02:24, 11.4MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  21% 430M/2.04G [00:08<01:29, 18.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  23% 461M/2.04G [00:09<00:59, 26.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  24% 482M/2.04G [00:09<00:45, 34.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  25% 514M/2.04G [00:09<00:31, 48.3MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  26% 535M/2.04G [00:09<00:25, 59.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  28% 566M/2.04G [00:09<00:18, 79.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  29% 587M/2.04G [00:09<00:15, 93.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  30% 608M/2.04G [00:09<00:13, 107MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  31% 629M/2.04G [00:09<00:11, 122MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  32% 650M/2.04G [00:10<00:10, 133MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  33% 671M/2.04G [00:10<00:09, 146MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  34% 692M/2.04G [00:10<00:08, 150MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  35% 713M/2.04G [00:10<00:08, 160MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  36% 734M/2.04G [00:10<00:07, 167MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  37% 755M/2.04G [00:10<00:11, 108MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  38% 776M/2.04G [00:13<00:56, 22.4MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  39% 797M/2.04G [00:13<00:41, 30.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  40% 818M/2.04G [00:13<00:30, 40.4MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  41% 839M/2.04G [00:13<00:22, 53.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  43% 870M/2.04G [00:14<00:15, 75.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  44% 902M/2.04G [00:14<00:11, 97.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  46% 933M/2.04G [00:14<00:09, 119MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  47% 954M/2.04G [00:14<00:08, 132MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  48% 975M/2.04G [00:14<00:07, 143MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  49% 1.01G/2.04G [00:14<00:06, 161MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  50% 1.03G/2.04G [00:14<00:05, 169MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  51% 1.05G/2.04G [00:14<00:05, 171MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  52% 1.07G/2.04G [00:15<00:05, 178MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  54% 1.10G/2.04G [00:15<00:05, 184MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  55% 1.12G/2.04G [00:15<00:07, 122MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  56% 1.14G/2.04G [00:16<00:10, 84.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  57% 1.16G/2.04G [00:19<00:43, 20.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  59% 1.20G/2.04G [00:19<00:27, 30.6MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  60% 1.22G/2.04G [00:19<00:20, 39.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  61% 1.25G/2.04G [00:19<00:14, 55.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  62% 1.27G/2.04G [00:19<00:11, 68.3MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  63% 1.29G/2.04G [00:19<00:09, 82.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  64% 1.31G/2.04G [00:19<00:07, 94.6MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  65% 1.33G/2.04G [00:19<00:06, 106MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  66% 1.35G/2.04G [00:20<00:05, 121MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  68% 1.38G/2.04G [00:20<00:04, 148MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  69% 1.42G/2.04G [00:20<00:03, 167MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  71% 1.45G/2.04G [00:20<00:03, 178MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  72% 1.47G/2.04G [00:20<00:03, 181MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  73% 1.49G/2.04G [00:20<00:04, 129MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  74% 1.51G/2.04G [00:25<00:33, 15.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  75% 1.53G/2.04G [00:25<00:24, 20.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  76% 1.55G/2.04G [00:25<00:17, 27.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  78% 1.58G/2.04G [00:25<00:10, 41.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  79% 1.60G/2.04G [00:26<00:08, 52.3MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  80% 1.64G/2.04G [00:26<00:05, 72.1MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  81% 1.66G/2.04G [00:26<00:04, 86.3MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  83% 1.69G/2.04G [00:26<00:03, 110MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  84% 1.72G/2.04G [00:26<00:02, 131MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  85% 1.74G/2.04G [00:26<00:02, 141MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  86% 1.76G/2.04G [00:26<00:01, 152MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  87% 1.78G/2.04G [00:26<00:01, 147MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  88% 1.80G/2.04G [00:27<00:01, 145MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  89% 1.82G/2.04G [00:27<00:01, 148MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  90% 1.85G/2.04G [00:27<00:01, 158MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  92% 1.87G/2.04G [00:27<00:01, 168MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  93% 1.89G/2.04G [00:27<00:01, 82.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  94% 1.91G/2.04G [00:31<00:07, 17.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  95% 1.94G/2.04G [00:31<00:03, 27.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  96% 1.96G/2.04G [00:31<00:02, 35.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  98% 1.99G/2.04G [00:31<00:00, 51.6MB/s]\u001b[A\n","model-00001-of-00002.safetensors: 100% 2.04G/2.04G [00:32<00:00, 63.7MB/s]\n","Downloading shards:  50% 1/2 [00:32<00:32, 32.21s/it]\n","model-00002-of-00002.safetensors:   0% 0.00/1.63G [00:00<?, ?B/s]\u001b[A\n","model-00002-of-00002.safetensors:   1% 10.5M/1.63G [00:00<01:29, 18.1MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   1% 21.0M/1.63G [00:00<01:00, 26.6MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   2% 31.5M/1.63G [00:01<00:44, 36.0MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   3% 41.9M/1.63G [00:01<00:34, 46.7MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   4% 62.9M/1.63G [00:01<00:23, 67.4MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   5% 83.9M/1.63G [00:01<00:17, 89.5MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   6% 105M/1.63G [00:01<00:14, 108MB/s]  \u001b[A\n","model-00002-of-00002.safetensors:   8% 126M/1.63G [00:01<00:11, 130MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  10% 157M/1.63G [00:01<00:09, 160MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  12% 189M/1.63G [00:02<00:09, 160MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  13% 210M/1.63G [00:05<01:08, 20.7MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  15% 241M/1.63G [00:05<00:45, 30.8MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  16% 262M/1.63G [00:05<00:35, 38.9MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  17% 283M/1.63G [00:05<00:27, 49.3MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  19% 304M/1.63G [00:05<00:21, 61.9MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  20% 325M/1.63G [00:06<00:17, 75.9MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  21% 346M/1.63G [00:06<00:14, 91.1MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  22% 367M/1.63G [00:06<00:11, 108MB/s] \u001b[A\n","model-00002-of-00002.safetensors:  24% 398M/1.63G [00:06<00:09, 133MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  26% 419M/1.63G [00:06<00:08, 141MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  27% 440M/1.63G [00:06<00:08, 149MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  28% 461M/1.63G [00:06<00:07, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  30% 482M/1.63G [00:06<00:06, 165MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  31% 503M/1.63G [00:07<00:06, 175MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  33% 535M/1.63G [00:07<00:06, 178MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  34% 556M/1.63G [00:13<01:30, 11.9MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  36% 587M/1.63G [00:13<00:57, 18.3MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  37% 608M/1.63G [00:13<00:43, 23.8MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  38% 629M/1.63G [00:13<00:32, 31.0MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  40% 650M/1.63G [00:13<00:24, 40.3MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  41% 671M/1.63G [00:14<00:18, 51.4MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  42% 692M/1.63G [00:14<00:14, 63.1MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  44% 713M/1.63G [00:14<00:11, 76.8MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  45% 734M/1.63G [00:14<00:09, 94.4MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  47% 765M/1.63G [00:14<00:07, 121MB/s] \u001b[A\n","model-00002-of-00002.safetensors:  48% 786M/1.63G [00:14<00:06, 130MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  49% 807M/1.63G [00:14<00:05, 139MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  51% 828M/1.63G [00:14<00:05, 145MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  52% 849M/1.63G [00:15<00:04, 158MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  54% 881M/1.63G [00:15<00:04, 174MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  55% 902M/1.63G [00:15<00:05, 133MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  56% 923M/1.63G [00:17<00:21, 32.7MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  58% 944M/1.63G [00:17<00:16, 42.7MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  60% 975M/1.63G [00:17<00:10, 61.8MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  62% 1.01G/1.63G [00:17<00:07, 81.9MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  63% 1.03G/1.63G [00:17<00:06, 96.1MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  64% 1.05G/1.63G [00:17<00:05, 111MB/s] \u001b[A\n","model-00002-of-00002.safetensors:  65% 1.07G/1.63G [00:18<00:04, 124MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  67% 1.09G/1.63G [00:18<00:03, 138MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  68% 1.11G/1.63G [00:18<00:03, 149MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  69% 1.13G/1.63G [00:18<00:03, 154MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  71% 1.15G/1.63G [00:18<00:02, 162MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  72% 1.18G/1.63G [00:18<00:02, 176MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  74% 1.21G/1.63G [00:18<00:02, 181MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  75% 1.23G/1.63G [00:18<00:02, 188MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  76% 1.25G/1.63G [00:19<00:02, 182MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  78% 1.27G/1.63G [00:19<00:02, 173MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  79% 1.29G/1.63G [00:19<00:01, 177MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  81% 1.32G/1.63G [00:19<00:01, 193MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  82% 1.34G/1.63G [00:19<00:01, 177MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  83% 1.36G/1.63G [00:19<00:01, 169MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  85% 1.38G/1.63G [00:19<00:01, 165MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  86% 1.41G/1.63G [00:19<00:01, 162MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  87% 1.43G/1.63G [00:20<00:01, 160MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  89% 1.45G/1.63G [00:20<00:01, 163MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  90% 1.47G/1.63G [00:20<00:01, 108MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  91% 1.49G/1.63G [00:21<00:02, 68.9MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  92% 1.50G/1.63G [00:25<00:11, 11.3MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  94% 1.53G/1.63G [00:25<00:05, 19.2MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  95% 1.55G/1.63G [00:25<00:03, 25.9MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  96% 1.57G/1.63G [00:26<00:01, 34.3MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  98% 1.59G/1.63G [00:26<00:00, 44.3MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  99% 1.61G/1.63G [00:26<00:00, 55.7MB/s]\u001b[A\n","model-00002-of-00002.safetensors: 100% 1.63G/1.63G [00:26<00:00, 61.6MB/s]\n","Downloading shards: 100% 2/2 [00:58<00:00, 29.48s/it]\n","[INFO|modeling_utils.py:1606] 2024-09-15 07:07:35,575 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.\n","[INFO|configuration_utils.py:1038] 2024-09-15 07:07:35,577 >> Generate config GenerationConfig {}\n","\n","Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.92s/it]\n","[INFO|modeling_utils.py:4507] 2024-09-15 07:07:41,817 >> All model checkpoint weights were used when initializing QWenLMHeadModel.\n","\n","[INFO|modeling_utils.py:4515] 2024-09-15 07:07:41,817 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at Qwen/Qwen-1_8B-Chat.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.\n","generation_config.json: 100% 249/249 [00:00<00:00, 1.19MB/s]\n","[INFO|configuration_utils.py:993] 2024-09-15 07:07:42,020 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/generation_config.json\n","[INFO|configuration_utils.py:1038] 2024-09-15 07:07:42,021 >> Generate config GenerationConfig {\n","  \"chat_format\": \"chatml\",\n","  \"do_sample\": true,\n","  \"eos_token_id\": 151643,\n","  \"max_new_tokens\": 512,\n","  \"max_window_size\": 6144,\n","  \"pad_token_id\": 151643,\n","  \"repetition_penalty\": 1.1,\n","  \"top_k\": 0,\n","  \"top_p\": 0.8\n","}\n","\n","09/15/2024 07:07:42 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n","09/15/2024 07:07:42 - INFO - llamafactory.model.loader - all params: 1,836,828,672\n","09/15/2024 07:07:42 - WARNING - llamafactory.chat.hf_engine - There is no current event loop, creating a new one.\n"]}]}]}